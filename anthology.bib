@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{kim2019cpcfg,
  author    = {Yoon Kim and
               Chris Dyer and
               Alexander M. Rush},
  title     = {Compound Probabilistic Context-Free Grammars for Grammar Induction},
  journal   = {CoRR},
  volume    = {abs/1906.10225},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.10225},
  archivePrefix = {arXiv},
  eprint    = {1906.10225},
  timestamp = {Thu, 27 Jun 2019 18:54:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-10225.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mikolov-2011,
  added-at = {2013-12-08T18:05:26.000+0100},
  author = {Mikolov, Tomas and Deoras, Anoop and Kombrink, Stefan and Burget, Lukás and Cernocký, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/2fd6741007f85845d79cbca2761fabe10/prlz77},
  ee = {http://www.isca-speech.org/archive/interspeech_2011/i11_0605.html},
  keywords = {Empirical advanced and combination evaluation language modeling of techniques},
  pages = {605-608},
  publisher = {ISCA},
  timestamp = {2013-12-08T18:05:26.000+0100},
  title = {Empirical Evaluation and Combination of Advanced Language Modeling Techniques.},
  url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2011.html#MikolovDKBC11},
  year = 2011
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  archivePrefix = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dedieu2019learning,
    title={Learning higher-order sequential structure with cloned HMMs},
    author={Antoine Dedieu and Nishad Gothoskar and Scott Swingle and Wolfgang Lehrach and Miguel Lázaro-Gredilla and Dileep George},
    year={2019},
    eprint={1905.00507},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
} 

@inproceedings{nepal2014fhmm,
  author    = {Anjan Nepal and
               Alexander Yates},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Factorial Hidden Markov Models for Learning Representations of Natural
               Language},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6168},
  timestamp = {Thu, 25 Jul 2019 14:36:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NepalY13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ghahramani1997fhmm,
author = {Ghahramani, Zoubin and Jordan, Michael I.},
title = {Factorial Hidden Markov Models},
year = {1997},
issue_date = {November 1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007425814087},
doi = {10.1023/A:1007425814087},
journal = {Mach. Learn.},
month = nov,
pages = {245–273},
numpages = {29},
keywords = {EM algorithm, mean field theory, Hidden Markov models, Bayesian networks, time series, graphical models}
}

@article{brown1992,
author = {Brown, Peter F. and deSouza, Peter V. and Mercer, Robert L. and Pietra, Vincent J. Della and Lai, Jenifer C.},
title = {Class-Based n-Gram Models of Natural Language},
year = {1992},
issue_date = {December 1992},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {18},
number = {4},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = dec,
pages = {467–479},
numpages = {13}
}

@article{ptb,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    url = "https://www.aclweb.org/anthology/J93-2004",
    pages = "313--330",
}

@article{wikitext,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  journal   = {CoRR},
  volume    = {abs/1609.07843},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07843},
  archivePrefix = {arXiv},
  eprint    = {1609.07843},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MerityXBS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bengio2003nlm,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}
  

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{merity2017awdlstm,
  author    = {Stephen Merity and
               Nitish Shirish Keskar and
               Richard Socher},
  title     = {Regularizing and Optimizing {LSTM} Language Models},
  journal   = {CoRR},
  volume    = {abs/1708.02182},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02182},
  archivePrefix = {arXiv},
  eprint    = {1708.02182},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-02182.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zaremba2014lstm,
  author    = {Wojciech Zaremba and
               Ilya Sutskever and
               Oriol Vinyals},
  title     = {Recurrent Neural Network Regularization},
  journal   = {CoRR},
  volume    = {abs/1409.2329},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.2329},
  archivePrefix = {arXiv},
  eprint    = {1409.2329},
  timestamp = {Mon, 13 Aug 2018 16:47:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZarembaSV14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mikolov2010rnn,
  added-at = {2015-04-09T00:00:00.000+0200},
  author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukás and Cernocký, Jan and Khudanpur, Sanjeev},
  biburl = {https://www.bibsonomy.org/bibtex/2aee1e280d06e82474b17c4996aaea076/dblp},
  booktitle = {INTERSPEECH},
  editor = {Kobayashi, Takao and Hirose, Keikichi and Nakamura, Satoshi},
  ee = {http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html},
  interhash = {9cabdcef2ed097906bff2cb7a327e61e},
  intrahash = {aee1e280d06e82474b17c4996aaea076},
  keywords = {dblp},
  pages = {1045-1048},
  publisher = {ISCA},
  timestamp = {2015-06-21T06:10:05.000+0200},
  title = {Recurrent neural network based language model.},
  url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10},
  year = 2010
}

@article{tran2016hmm,
  author    = {Ke M. Tran and
               Yonatan Bisk and
               Ashish Vaswani and
               Daniel Marcu and
               Kevin Knight},
  title     = {Unsupervised Neural Hidden Markov Models},
  journal   = {CoRR},
  volume    = {abs/1609.09007},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.09007},
  archivePrefix = {arXiv},
  eprint    = {1609.09007},
  timestamp = {Sat, 28 Sep 2019 00:58:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TranBVMK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bowman2015vae,
  author    = {Samuel R. Bowman and
               Luke Vilnis and
               Oriol Vinyals and
               Andrew M. Dai and
               Rafal J{\'{o}}zefowicz and
               Samy Bengio},
  title     = {Generating Sentences from a Continuous Space},
  journal   = {CoRR},
  volume    = {abs/1511.06349},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06349},
  archivePrefix = {arXiv},
  eprint    = {1511.06349},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BowmanVVDJB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chung2015vrnn,
  author    = {Junyoung Chung and
               Kyle Kastner and
               Laurent Dinh and
               Kratarth Goel and
               Aaron C. Courville and
               Yoshua Bengio},
  title     = {A Recurrent Latent Variable Model for Sequential Data},
  journal   = {CoRR},
  volume    = {abs/1506.02216},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02216},
  archivePrefix = {arXiv},
  eprint    = {1506.02216},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungKDGCB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{krakovna2016hmm,
    title={Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models},
    author={Viktoriya Krakovna and Finale Doshi-Velez},
    year={2016},
    eprint={1606.05320},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{buys2018hmm,
  title={Bridging HMMs and RNNs through Architectural Transformations},
  author={Jan Buys and Yonatan Bisk and Yejin Choi},
  year={2018}
}

@article{stratos2016ahmm,
    title = "Unsupervised Part-Of-Speech Tagging with Anchor Hidden {M}arkov Models",
    author = "Stratos, Karl  and
      Collins, Michael  and
      Hsu, Daniel",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1018",
    doi = "10.1162/tacl_a_00096",
    pages = "245--257",
    abstract = "We tackle unsupervised part-of-speech (POS) tagging by learning hidden Markov models (HMMs) that are particularly well-suited for the problem. These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., {``}the{''} is a word that appears only under the determiner tag). We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al. (2013) to design a consistent estimator for anchor HMMs. In experiments, our algorithm is competitive with strong baselines such as the clustering method of Brown et al. (1992) and the log-linear model of Berg-Kirkpatrick et al. (2010). Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words.",
}

@article{kim2019urnng,
  author    = {Yoon Kim and
               Alexander M. Rush and
               Lei Yu and
               Adhiguna Kuncoro and
               Chris Dyer and
               G{\'{a}}bor Melis},
  title     = {Unsupervised Recurrent Neural Network Grammars},
  journal   = {CoRR},
  volume    = {abs/1904.03746},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.03746},
  archivePrefix = {arXiv},
  eprint    = {1904.03746},
  timestamp = {Thu, 25 Apr 2019 13:55:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-03746.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nepal2013fhmm,
  title={Factorial Hidden Markov Models for Learning Representations of Natural Language},
  author={Anjan Nepal and Alexander Yates},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6168}
}

@article{zoubin1997fhmm,
author = {Ghahramani, Zoubin and Jordan, Michael I.},
title = {Factorial Hidden Markov Models},
year = {1997},
issue_date = {Nov./Dec. 1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007425814087},
doi = {10.1023/A:1007425814087},
journal = {Mach. Learn.},
month = nov,
pages = {245–273},
numpages = {29},
keywords = {mean field theory, Hidden Markov models, EM algorithm, graphical models, time series, Bayesian networks}
}
  

@article{kuhn1994hmmlm,
  added-at = {2020-02-27T00:00:00.000+0100},
  author = {Kuhn, Thomas and Niemann, Heinrich and Schukat-Talamazzini, Ernst Günter},
  biburl = {https://www.bibsonomy.org/bibtex/2f169c950833f0fde2684a78ce0dc013b/dblp},
  booktitle = {ICASSP (1)},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICASSP.1994.389282},
  interhash = {b9eb9fbfbb9dfd3eef6a98802102f5b3},
  intrahash = {f169c950833f0fde2684a78ce0dc013b},
  isbn = {0-7803-1775-0},
  keywords = {dblp},
  pages = {357-360},
  publisher = {IEEE Computer Society},
  timestamp = {2020-02-28T11:41:02.000+0100},
  title = {Ergodic hidden Markov models and polygrams for language modeling.},
  url = {http://dblp.uni-trier.de/db/conf/icassp/icassp1994.html#KuhnNS94},
  year = 1994
}

@article{huang2009poshmm,
    title = "Improving A Simple Bigram {HMM} Part-of-Speech Tagger by Latent Annotation and Self-Training",
    author = "Huang, Zhongqiang  and
      Eidelman, Vladimir  and
      Harper, Mary",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N09-2054",
    pages = "213--216",
}

@phdthesis{huang2011thesis,
  author       = {Zhongqiang Huang}, 
  title        = {Modeling Dependencies in Natural Languages with Latent Variables},
  school       = {University of Maryland},
  year         = 2011,
    url = {https://drum.lib.umd.edu/handle/1903/12295},
}

@article{petrov2006splitmerge,
author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1220175.1220230},
doi = {10.3115/1220175.1220230},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
pages = {433–440},
numpages = {8},
location = {Sydney, Australia},
series = {ACL-44}
}
  
@inproceedings{vogel1996hmm,
author = {Vogel, Stephan and Ney, Hermann and Tillmann, Christoph},
title = {HMM-Based Word Alignment in Statistical Translation},
year = {1996},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/993268.993313},
doi = {10.3115/993268.993313},
booktitle = {Proceedings of the 16th Conference on Computational Linguistics - Volume 2},
pages = {836–841},
numpages = {6},
location = {Copenhagen, Denmark},
series = {COLING ’96}
}
  

@inbook{rabiner1990tut,
author = {Rabiner, Lawrence R.},
title = {A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
year = {1990},
isbn = {1558601244},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Readings in Speech Recognition},
pages = {267–296},
numpages = {30}
}

@article{bradbury2016qrnn,
  author    = {James Bradbury and
               Stephen Merity and
               Caiming Xiong and
               Richard Socher},
  title     = {Quasi-Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1611.01576},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01576},
  archivePrefix = {arXiv},
  eprint    = {1611.01576},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BradburyMXS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ma2016crf,
  author    = {Xuezhe Ma and
               Eduard H. Hovy},
  title     = {End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},
  journal   = {CoRR},
  volume    = {abs/1603.01354},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.01354},
  archivePrefix = {arXiv},
  eprint    = {1603.01354},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MaH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{mikolov2012rnn,

  author={T. {Mikolov} and G. {Zweig}},

  booktitle={2012 IEEE Spoken Language Technology Workshop (SLT)}, 

  title={Context dependent recurrent neural network language model}, 

  year={2012},

  volume={},

  number={},

  pages={234-239},}

@inproceedings{han2017dependency,
    title = "Dependency Grammar Induction with Neural Lexicalization and Big Training Data",
    author = "Han, Wenjuan  and
      Jiang, Yong  and
      Tu, Kewei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1176",
    doi = "10.18653/v1/D17-1176",
    pages = "1683--1688",
    abstract = "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.",
}

@article{wiseman2018hsmm,
  author    = {Sam Wiseman and
               Stuart M. Shieber and
               Alexander M. Rush},
  title     = {Learning Neural Templates for Text Generation},
  journal   = {CoRR},
  volume    = {abs/1808.10122},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.10122},
  archivePrefix = {arXiv},
  eprint    = {1808.10122},
  timestamp = {Mon, 03 Sep 2018 13:36:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-10122.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kim2015charcnn,
  author    = {Yoon Kim and
               Yacine Jernite and
               David A. Sontag and
               Alexander M. Rush},
  title     = {Character-Aware Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/1508.06615},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.06615},
  archivePrefix = {arXiv},
  eprint    = {1508.06615},
  timestamp = {Fri, 15 Nov 2019 17:16:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/KimJSR15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ladner1980prefix,
author = {Ladner, Richard E. and Fischer, Michael J.},
title = {Parallel Prefix Computation},
year = {1980},
issue_date = {Oct. 1980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/322217.322232},
doi = {10.1145/322217.322232},
journal = {J. ACM},
month = oct,
pages = {831–838},
numpages = {8}
}
  

@INPROCEEDINGS{liang2005brown,
    author = {Percy Liang},
    title = {Semi-supervised learning for natural language},
    booktitle = {MASTER’S THESIS, MIT},
    year = {2005},
    publisher = {}
}

@inproceedings{kenlm,
  title = {Scalable Modified {K}neser-{N}ey Language Model Estimation},
  author = {Heafield, Kenneth and Pouzyrevsky, Ivan and Clark, Jonathan H. and Koehn, Philipp},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month = aug,
  year = {2013},
  address = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  url = {https://www.aclweb.org/anthology/P13-2121},
  pages = {690--696},
  month_numeric = {8}
}

@article{awddoc,
  author    = {Sho Takase and
               Jun Suzuki and
               Masaaki Nagata},
  title     = {Direct Output Connection for a High-Rank Language Model},
  journal   = {CoRR},
  volume    = {abs/1808.10143},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.10143},
  archivePrefix = {arXiv},
  eprint    = {1808.10143},
  timestamp = {Mon, 03 Sep 2018 13:36:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-10143.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mos,
  author    = {Zhilin Yang and
               Zihang Dai and
               Ruslan Salakhutdinov and
               William W. Cohen},
  title     = {Breaking the Softmax Bottleneck: {A} High-Rank {RNN} Language Model},
  journal   = {CoRR},
  volume    = {abs/1711.03953},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.03953},
  archivePrefix = {arXiv},
  eprint    = {1711.03953},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-03953.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xu2018overparam,
  author    = {Ji Xu and
               Daniel Hsu and
               Arian Maleki},
  title     = {Benefits of over-parameterization with {EM}},
  journal   = {CoRR},
  volume    = {abs/1810.11344},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.11344},
  archivePrefix = {arXiv},
  eprint    = {1810.11344},
  timestamp = {Wed, 31 Oct 2018 14:24:29 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-11344.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tvm,
  author    = {Tianqi Chen and
               Thierry Moreau and
               Ziheng Jiang and
               Haichen Shen and
               Eddie Q. Yan and
               Leyuan Wang and
               Yuwei Hu and
               Luis Ceze and
               Carlos Guestrin and
               Arvind Krishnamurthy},
  title     = {{TVM:} End-to-End Optimization Stack for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1802.04799},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.04799},
  archivePrefix = {arXiv},
  eprint    = {1802.04799},
  timestamp = {Mon, 13 Aug 2018 16:47:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-04799.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article {clonedgraphs,
	Title = {Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps},
	Author = {George, Dileep and Rikhye, Rajeev V and Gothoskar, Nishad and Guntupalli, J Swaroop and Dedieu, Antoine and Lázaro-Gredilla, Miguel},
	DOI = {10.1038/s41467-021-22559-5},
	Number = {1},
	Volume = {12},
	Month = {April},
	Year = {2021},
	Journal = {Nature communications},
	ISSN = {2041-1723},
	Pages = {2392},
	Abstract = {Cognitive maps are mental representations of spatial and conceptual relationships in an environment, and are critical for flexible behavior. To form these abstract maps, the hippocampus has to learn to separate or merge aliased observations appropriately in different contexts in a manner that enables generalization and efficient planning. Here we propose a specific higher-order graph structure, clone-structured cognitive graph (CSCG), which forms clones of an observation for different contexts as a representation that addresses these problems. CSCGs can be learned efficiently using a probabilistic sequence model that is inherently robust to uncertainty. We show that CSCGs can explain a variety of cognitive map phenomena such as discovering spatial relations from aliased sensations, transitive inference between disjoint episodes, and formation of transferable schemas. Learning different clones for different contexts explains the emergence of splitter cells observed in maze navigation and event-specific responses in lap-running experiments. Moreover, learning and inference dynamics of CSCGs offer a coherent explanation for disparate place cell remapping phenomena. By lifting aliased observations into a hidden space, CSCGs reveal latent modularity useful for hierarchical abstraction and planning. Altogether, CSCG provides a simple unifying framework for understanding hippocampal function, and could be a pathway for forming relational abstractions in artificial intelligence.},
	URL = {https://doi.org/10.1038/s41467-021-22559-5},
}

@inproceedings{dmc,
author = {Siddiqi, Sajid M. and Moore, Andrew W.},
title = {Fast Inference and Learning in Large-State-Space HMMs},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.library.cornell.edu/10.1145/1102351.1102452},
doi = {10.1145/1102351.1102452},
abstract = {For Hidden Markov Models (HMMs) with fully connected transition models, the three fundamental problems of evaluating the likelihood of an observation sequence, estimating an optimal state sequence for the observations, and learning the model parameters, all have quadratic time complexity in the number of states. We introduce a novel class of non-sparse Markov transition matrices called Dense-Mostly-Constant (DMC) transition matrices that allow us to derive new algorithms for solving the basic HMM problems in sub-quadratic time. We describe the DMC HMM model and algorithms and attempt to convey some intuition for their usage. Empirical results for these algorithms show dramatic speedups for all three problems. In terms of accuracy, the DMC model yields strong results and outperforms the baseline algorithms even in domains known to violate the DMC assumption.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {800–807},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@inproceedings{ffthmm,
 author = {Felzenszwalb, Pedro and Huttenlocher, Daniel and Kleinberg, Jon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis},
 url = {https://proceedings.neurips.cc/paper/2003/file/9407c826d8e3c07ad37cb2d13d1cb641-Paper.pdf},
 volume = {16},
 year = {2004}
}

@MISC{convhmm,
    author = {Javier R. Movellan and John Hershey and Josh Susskind},
    title = {Real-Time Video Tracking Using Convolution HMMs},
    year = {2004}
}

@ARTICLE{46546,

  author={Lee, K.-F. and Hon, H.-W.},

  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 

  title={Speaker-independent phone recognition using hidden Markov models}, 

  year={1989},

  volume={37},

  number={11},

  pages={1641-1648},

  doi={10.1109/29.46546}}

@article{merialdo1994tagging,
    title = "Tagging {E}nglish Text with a Probabilistic Model",
    author = "Merialdo, Bernard",
    journal = "Computational Linguistics",
    volume = "20",
    number = "2",
    year = "1994",
    url = "https://www.aclweb.org/anthology/J94-2001",
    pages = "155--171",
}

@article{switch,
  author    = {Jakob N. Foerster and
               Justin Gilmer and
               Jan Chorowski and
               Jascha Sohl{-}Dickstein and
               David Sussillo},
  title     = {Intelligible Language Modeling with Input Switched Affine Networks},
  journal   = {CoRR},
  volume    = {abs/1611.09434},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09434},
  archivePrefix = {arXiv},
  eprint    = {1611.09434},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FoersterGCSS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lveg,
    title = "{G}aussian Mixture Latent Vector Grammars",
    author = "Zhao, Yanpeng  and
      Zhang, Liwen  and
      Tu, Kewei",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1109",
    doi = "10.18653/v1/P18-1109",
    pages = "1181--1189",
    abstract = "We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.",
}

@inproceedings{
kal,
title={Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps},
author={Tri Dao and Nimit Sohoni and Albert Gu and Matthew Eichhorn and Amit Blonder and Megan Leszczynski and Atri Rudra and Christopher Ré},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkgrBgSYDS}
}

@inproceedings{embhmm,
author = {Neal, Radford M. and Beal, Matthew J. and Roweis, Sam T.},
title = {Inferring State Sequences for Non-Linear Systems with Embedded Hidden Markov Models},
year = {2003},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of "pools" of candidate states at each time. We then define an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers.},
booktitle = {Proceedings of the 16th International Conference on Neural Information Processing Systems},
pages = {401–408},
numpages = {8},
location = {Whistler, British Columbia, Canada},
series = {NIPS'03}
}

@misc{performer,
      title={Rethinking Attention with Performers}, 
      author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
      year={2021},
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rfa,
      title={Random Feature Attention}, 
      author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah A. Smith and Lingpeng Kong},
      year={2021},
      eprint={2103.02143},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{blanc2018adaptive,
      title={Adaptive Sampled Softmax with Kernel Based Sampling}, 
      author={Guy Blanc and Steffen Rendle},
      year={2018},
      eprint={1712.00527},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dhmm,
  author    = {Maoying Qiao and
               Wei Bian and
               Richard Yida Xu and
               Dacheng Tao},
  title     = {Diversified Hidden Markov Models for Sequential Labeling},
  journal   = {CoRR},
  volume    = {abs/1904.03170},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.03170},
  archivePrefix = {arXiv},
  eprint    = {1904.03170},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-03170.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{enlm,
  author    = {Yangfeng Ji and
               Chenhao Tan and
               Sebastian Martschat and
               Yejin Choi and
               Noah A. Smith},
  title     = {Dynamic Entity Representations in Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/1708.00781},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.00781},
  archivePrefix = {arXiv},
  eprint    = {1708.00781},
  timestamp = {Mon, 13 Aug 2018 16:47:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-00781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lda,
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  added-at = {2010-03-23T16:09:41.000+0100},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  biburl = {https://www.bibsonomy.org/bibtex/2bc34cc810fa7dfa12b949b60c23d9f5c/zhenzhenx},
  description = {Latent dirichlet allocation},
  doi = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
  interhash = {9d1b808272b9e511425cbf557571e59a},
  intrahash = {bc34cc810fa7dfa12b949b60c23d9f5c},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  keywords = {LDA allocation dirichlet latent},
  pages = {993--1022},
  publisher = {JMLR.org},
  timestamp = {2010-06-16T11:05:42.000+0200},
  title = {Latent dirichlet allocation},
  url = {http://portal.acm.org/citation.cfm?id=944937},
  volume = 3,
  year = 2003
}



